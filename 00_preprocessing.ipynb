{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa54e0e-70ed-4692-9c3e-969249ae2b7b",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "*Tim Braams (8460701), Vinh PhanÂ (8462380), Maximilian Pintilie (8462780), Rahul Singh (8464147), Kartik Vijay (8463465), Diego Zucchino (8345420)*   \n",
    "  \n",
    "Currently we have minute data for 10 cryptos. To reduce the data size we convert the data to hourly by taking the volume weighted average price of that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd026b0-51fa-477c-b240-defeb1504936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import scripts.utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6e006-b899-4688-a5a5-4ebde117a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data not provided with this submission, please put it into Crypto_data_minute folder in this directory\n",
    "files =  [file for file in os.listdir(\"data/Crypto_data_minute\") if file.split(\".\")[1] == \"txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1b767-ea33-4269-83ee-2d10f1ac77ae",
   "metadata": {},
   "source": [
    "#### Data resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c386f-8f10-44b5-9ff7-c57ac162ad22",
   "metadata": {},
   "source": [
    "##### Equidistant data\n",
    "First we create frames for each crypto that have a continous timeseries (equidistant timestamps). This data will be used for the Darts libary. If a hour had no trades, we forward fill the last observed price for the hour without prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe34c2-a84a-4bcb-9027-3bb267950682",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files: \n",
    "    name = file.split(\".\")[0].split(\"_\")[0]\n",
    "    df_full = pd.read_csv(f\"data/Crypto_data_minute/{file}\", header=None)\n",
    "    df_full.columns = [\"Open Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "    df_full = df_full.sort_values(\"Open Time\")\n",
    "    df_full[\"Open Time\"] = pd.to_datetime(df_full[\"Open Time\"])\n",
    "    df_full[\"volume_weighted\"] = df_full[\"Close\"] * df_full[\"Volume\"]\n",
    "    df_resample = df_full.resample(\"1h\", on=\"Open Time\").sum()\n",
    "    df_resample[\"wClose\"] = df_resample[\"volume_weighted\"] / df_resample[\"Volume\"]\n",
    "    df_resample = df_resample.fillna(method=\"ffill\")\n",
    "    df_resample.to_json(f\"data/Resample/{name}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41987d2-4cf3-450e-8d39-79b233471295",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = ut.read_and_concat(\"data/Resample\")\n",
    "master_table.to_csv(\"data/master_returns_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caebd14e-87fb-4691-b79a-95c408d16142",
   "metadata": {},
   "source": [
    "##### Raw data (non-equidistant)\n",
    "Next we do the same but without forward filling. Therefore, if a hour had no trades, it will not show up in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857fa7c-12bf-446f-b112-eeddb4323b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files: \n",
    "    name = file.split(\".\")[0].split(\"_\")[0]\n",
    "    df_full = pd.read_csv(f\"data/Crypto_data_minute/{file}\", header=None)\n",
    "    df_full.columns = [\"Open Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "    df_full = df_full.sort_values(\"Open Time\")\n",
    "    df_full[\"Open Time\"] = pd.to_datetime(df_full[\"Open Time\"])\n",
    "    df_full[\"volume_weighted\"] = df_full[\"Close\"] * df_full[\"Volume\"]\n",
    "    df_resample = df_full.resample(\"1h\", on=\"Open Time\").sum()\n",
    "    df_resample[\"wClose\"] = df_resample[\"volume_weighted\"] / df_resample[\"Volume\"]\n",
    "    df_resample = df_resample.dropna(subset=[\"wClose\"])\n",
    "    df_resample.to_json(f\"data/Resample_non_equidistant/{name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b9735-6cda-42d8-bce8-21d2fde61e79",
   "metadata": {},
   "source": [
    "#### Data pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc882198-e398-47ec-a479-d06c75d16ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee7375-fa0e-4e52-8e6d-3c00f99db8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files =  [file for file in os.listdir(\"data/Resample_non_equidistant\") if file.split(\".\")[1] == \"json\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56a7e3-1f6b-4a75-9255-4a23c829ebd5",
   "metadata": {},
   "source": [
    "The pooled data will have a column for returns and a window of returns preceding that timestamp (each timestamp in the window has one column t1, ..., tn). One final column marks the coin the table is generated for. In the end all cryptos are concatiated into one big frame. This looks like to following:\n",
    "\n",
    "|            | returns | t1   | t2   | t3   | asset |\n",
    "|------------|---------|------|------|------|-------|\n",
    "| 2022-01-01 | 0.2     | 0.1  | 0.05 | 0.03 | BTC   |\n",
    "| 2022-01-02 | 0.1     | 0.2  | 0.1  | 0.05 | BTC   |\n",
    "| 2022-01-03 | 0.15    | 0.1  | 0.2  | 0.1  | BTC   |\n",
    "| 2022-01-01 | 0.05    | 0.01 | 0.5  | 0.33 | ETH   |\n",
    "| 2022-01-02 | 0.3     | 0.05 | 0.01 | 0.5  | ETH   |\n",
    "| 2022-01-03 | 0.1     | 0.3  | 0.05 | 0.01 | ETH   |\n",
    "\n",
    "All returns are calculted in relation to the next timestamp (i.e. about hourly returns - if all timestamps are available). The window was choosen based on EDA (next notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc606a0-cad0-4fb2-86bf-5039f0030083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(df, timedelta: int = 1):\n",
    "    df[\"returns\"] = ((df[\"wClose\"] / df[\"wClose\"].shift(timedelta)) - 1)\n",
    "    return df.dropna(subset=[\"returns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb74de55-73b6-460b-87b1-7df29a6aba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shifted_frame(file, window=100, norm=True):\n",
    "    name = file.split(\".\")[0]\n",
    "    df_full = pd.read_json(f\"data/Resample_non_equidistant/{name}.json\")\n",
    "    df_temp = get_returns(df_full, timedelta=1).copy()\n",
    "    train, test = get_train_test(df_temp)\n",
    "    if norm:\n",
    "        train_norm = pd.DataFrame((train[\"returns\"]-train[\"returns\"].mean())/train[\"returns\"].std())\n",
    "        test_norm = pd.DataFrame((test[\"returns\"]-test[\"returns\"].mean())/test[\"returns\"].std())\n",
    "        \n",
    "        train_norm[\"mean\"] = train[\"returns\"].mean()\n",
    "        train_norm[\"std\"] = train[\"returns\"].std()\n",
    "        test_norm[\"mean\"] = test[\"returns\"].mean()\n",
    "        test_norm[\"std\"] = test[\"returns\"].std()\n",
    "        \n",
    "        returns = pd.concat([train_norm, test_norm])\n",
    "        returns.index = df_temp.index\n",
    "        \n",
    "    else:\n",
    "        returns = df_temp[\"returns\"]\n",
    "    \n",
    "    if norm:\n",
    "        columns = [returns[\"returns\"], returns[\"mean\"], returns[\"std\"]]\n",
    "    else: \n",
    "        columns = [returns]\n",
    "    for n in range(window):\n",
    "        if norm:\n",
    "            series_new = returns[\"returns\"].shift(n + 1)\n",
    "        else:\n",
    "            series_new = returns.shift(n + 1)\n",
    "        series_new.name = f\"t{n+1}\"\n",
    "        columns.append(series_new)\n",
    "    concat_frame = pd.concat(columns, axis=1)\n",
    "    concat_frame[\"asset\"] = name\n",
    "    return concat_frame\n",
    "\n",
    "def get_train_test(frame):\n",
    "    cutoff_ts = frame.index[-1] - pd.Timedelta(365, \"days\")\n",
    "    train = frame[frame.index < cutoff_ts].copy()\n",
    "    test = frame[frame.index >= cutoff_ts].copy()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91015a4-e994-464a-9d30-6637bd784003",
   "metadata": {},
   "source": [
    "We choose to not normalize the data, as we observed in the EDA that the returns are mean centred and therefore normilazation would not greatly benefit the models. Individual models might have normilazation implemented (see simple LSTM), for example as a layer, if the model benefitted from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107339f-c785-4026-b607-1607b7d306d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_train = []\n",
    "frames_test = []\n",
    "for file in files:\n",
    "    print(file)\n",
    "    frame = get_shifted_frame(file=file, window=300, norm=False)\n",
    "    train, test = get_train_test(frame)\n",
    "    frames_train.append(train)\n",
    "    frames_test.append(test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a56f691-a87f-47cb-8977-429c2e161a37",
   "metadata": {},
   "source": [
    "Save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b339e2-f2ee-446b-ba55-b2f65404f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat(frames_train, axis=0).dropna()\n",
    "test_full = pd.concat(frames_test, axis=0).dropna()\n",
    "\n",
    "train_full.reset_index().to_csv(\"data/pooled_train_300.csv\")\n",
    "test_full.reset_index().to_csv(\"data/pooled_test_300.csv\")\n",
    "train_full.reset_index().to_json(\"data/pooled_train_300.json\", date_unit=\"ns\")\n",
    "test_full.reset_index().to_json(\"data/pooled_test_300.json\", date_unit=\"ns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
