{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: LSTM with Attention\n",
    "This model we unfortunatly where not able to get to work properly in the given timeframe. We still included it for completeness, if we had more time we would try to understand it more and get it to predict properly. Initial results look promising even though the performance is not as good as the other models presented so far. \n",
    "We also faced some hardware issues (mainly vRAM) which made it unfeasable to continue with the model for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GQ32sz7gXSba"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from pickle import dump,load\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from tensorflow.keras.layers import Normalization\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Permute\n",
    "from keras.layers import Input, Dense, Activation, RepeatVector, add ,Flatten, TimeDistributed, Multiply\n",
    "from keras.layers import Embedding, LSTM, BatchNormalization, dot, concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GQcZxbIxXDDI"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/pooled_train_300.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D1h2EN2oXOO8"
   },
   "outputs": [],
   "source": [
    "# train.insert(0, \"open_time \", pd.to_datetime(train[\"index\"]))\n",
    "train.sort_values(by='index', inplace = True)\n",
    "\n",
    "#keep train where train zero > 50%\n",
    "train[\"zeros_perc\"] = (train == 0).astype(int).sum(axis=1).div(train.shape[1]-4)\n",
    "train = train[train[\"zeros_perc\"]< 0.3]\n",
    "\n",
    "# train, val= ()\n",
    "train_1, val_train_1 = train_test_split(train, test_size=0.2, random_state=42, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7wSEM2iAY-bA"
   },
   "outputs": [],
   "source": [
    "y_train = train_1[\"returns\"]\n",
    "X_train = train_1.drop([\"index\", \"asset\", \"returns\", \"Unnamed: 0\", \"zeros_perc\"], axis=1)\n",
    "\n",
    "# val train: \n",
    "y_val = val_train_1[\"returns\"]\n",
    "X_val = val_train_1.drop([\"index\", \"asset\", \"returns\", \"Unnamed: 0\", \"zeros_perc\"], axis=1)\n",
    "\n",
    "\n",
    "# y_test = test[\"returns\"]\n",
    "# X_test = test.drop([\"index\", \"asset\", \"returns\", \"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0UWzfIaSZAfM"
   },
   "outputs": [],
   "source": [
    "X_train= X_train.to_numpy().reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "y_train= y_train.to_numpy().reshape(y_train.shape[0],1)\n",
    "\n",
    "X_val_train= X_val.to_numpy().reshape(X_val.shape[0],X_val.shape[1],1)\n",
    "y_val_train= y_val.to_numpy().reshape(y_val.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uATS0xF4tS-D",
    "outputId": "44389872-064c-4058-94d9-059e840a5673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 300, 400), dtype=tf.float32, name=None), name='lstm_16/PartitionedCall:1', description=\"created by layer 'lstm_16'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='lstm_16/PartitionedCall:2', description=\"created by layer 'lstm_16'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='lstm_16/PartitionedCall:3', description=\"created by layer 'lstm_16'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 400), dtype=tf.float32, name=None), name='repeat_vector_8/Tile:0', description=\"created by layer 'repeat_vector_8'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 400), dtype=tf.float32, name=None), name='lstm_17/PartitionedCall:1', description=\"created by layer 'lstm_17'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 300), dtype=tf.float32, name=None), name='activation_8/Softmax:0', description=\"created by layer 'activation_8'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 400), dtype=tf.float32, name=None), name='batch_normalization_26/batchnorm/add_1:0', description=\"created by layer 'batch_normalization_26'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 800), dtype=tf.float32, name=None), name='concatenate_8/concat:0', description=\"created by layer 'concatenate_8'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None), name='time_distributed_8/Reshape_1:0', description=\"created by layer 'time_distributed_8'\")\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 300, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 [(None, 300, 400),   643200      ['input_17[0][0]']               \n",
      "                                 (None, 400),                                                     \n",
      "                                 (None, 400)]                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 400)         1600        ['lstm_16[0][1]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " repeat_vector_8 (RepeatVector)  (None, 1, 400)      0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 400)         1600        ['lstm_16[0][2]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)                 (None, 1, 400)       1281600     ['repeat_vector_8[0][0]',        \n",
      "                                                                  'batch_normalization_24[0][0]', \n",
      "                                                                  'batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dot_16 (Dot)                   (None, 1, 300)       0           ['lstm_17[0][0]',                \n",
      "                                                                  'lstm_16[0][0]']                \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 1, 300)       0           ['dot_16[0][0]']                 \n",
      "                                                                                                  \n",
      " dot_17 (Dot)                   (None, 1, 400)       0           ['activation_8[0][0]',           \n",
      "                                                                  'lstm_16[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 1, 400)      1600        ['dot_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 1, 800)       0           ['batch_normalization_26[0][0]', \n",
      "                                                                  'lstm_17[0][0]']                \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDistri  (None, 1, 1)        801         ['concatenate_8[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,930,401\n",
      "Trainable params: 1,928,001\n",
      "Non-trainable params: 2,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#SOURCE: https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/\n",
    "#INPUT layer\n",
    "n_hidden = 400\n",
    "input_train = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "output_train = Input(shape=(y_train.shape[1], 1))\n",
    "\n",
    "# ENCODER LSTM (returns last hidden and last cell state and (new due to attention alignment score))\n",
    "encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n",
    "    n_hidden, activation='tanh', dropout=0.0, recurrent_dropout=0, \n",
    "    return_state=True, return_sequences=True)(input_train)\n",
    "print(encoder_stack_h)\n",
    "print(encoder_last_h)\n",
    "print(encoder_last_c)\n",
    "### add batchnorm to avoid gradient explosion\n",
    "encoder_last_h = BatchNormalization(momentum=0.4)(encoder_last_h)\n",
    "encoder_last_c = BatchNormalization(momentum=0.4)(encoder_last_c)\n",
    "\n",
    "# DEDODER LSTM\n",
    "decoder_input = RepeatVector(output_train.shape[1])(encoder_last_h)\n",
    "print(decoder_input)\n",
    "### alignement score\n",
    "####stacked hidden state\n",
    "decoder_stack_h = LSTM(n_hidden, activation='tanh', dropout=0.0, recurrent_dropout=0,\n",
    " return_state=False, return_sequences=True)(\n",
    " decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
    "print(decoder_stack_h)\n",
    "\n",
    "# ATTENTION layer\n",
    "attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
    "attention = Activation('softmax')(attention)\n",
    "print(attention)\n",
    "### context vector and batch normalization\n",
    "context = dot([attention, encoder_stack_h], axes=[2,1])\n",
    "context = BatchNormalization(momentum=0.4)(context)\n",
    "print(context)\n",
    "### concat context and stacked hidden states\n",
    "decoder_combined_context = concatenate([context, decoder_stack_h])\n",
    "print(decoder_combined_context)\n",
    "out = TimeDistributed(Dense(output_train.shape[2]))(decoder_combined_context)\n",
    "print(out)\n",
    "\n",
    "# COMPILE\n",
    "model = Model(inputs=input_train, outputs=out)\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, clipnorm=1)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-7VclZ0T-95",
    "outputId": "10758461-3117-4e46-e084-750bb67d3525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 146s 721ms/step - loss: 0.0044 - mae: 0.0092 - val_loss: 3.3158e-04 - val_mae: 0.0156\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 142s 712ms/step - loss: 3.1909e-04 - mae: 0.0077 - val_loss: 1.3832e-04 - val_mae: 0.0078\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 143s 713ms/step - loss: 1.6820e-04 - mae: 0.0076 - val_loss: 1.1691e-04 - val_mae: 0.0063\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 143s 713ms/step - loss: 1.6907e-04 - mae: 0.0076 - val_loss: 2.4708e-04 - val_mae: 0.0128\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 142s 712ms/step - loss: 1.7243e-04 - mae: 0.0077 - val_loss: 1.2101e-04 - val_mae: 0.0066\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 142s 712ms/step - loss: 1.6907e-04 - mae: 0.0077 - val_loss: 1.1600e-04 - val_mae: 0.0062\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 142s 711ms/step - loss: 1.6815e-04 - mae: 0.0076 - val_loss: 1.1582e-04 - val_mae: 0.0061\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 142s 712ms/step - loss: 1.6850e-04 - mae: 0.0076 - val_loss: 1.2358e-04 - val_mae: 0.0068\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 142s 711ms/step - loss: 1.6997e-04 - mae: 0.0077 - val_loss: 3.9953e-04 - val_mae: 0.0177\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 143s 714ms/step - loss: 1.7085e-04 - mae: 0.0077 - val_loss: 1.4392e-04 - val_mae: 0.0081\n"
     ]
    }
   ],
   "source": [
    "epc = 100\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
    "history = model.fit(X_train[:, :], y_train[:, :], validation_split=0.2, \n",
    "                    epochs=epc, verbose=1, callbacks=[es], \n",
    "                    batch_size=1200)\n",
    "train_mae = history.history['mae']\n",
    "valid_mae = history.history['val_mae']\n",
    " \n",
    "model.save('model_forecasting_seq2seq7.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our working notes so far:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "X1DKfH4RS2pV",
    "outputId": "ccfccf4b-6de3-44d1-a7a2-e23d7a67b890"
   },
   "source": [
    "we started with stacked lstm and found 2 layers perform well.\n",
    "we also found results where attention improves the model\n",
    "\n",
    "increasing batch size from 32 to 64 to 128 to 500 to 1000 to 2000 to 4000\n",
    "reduce recurrent dropout to 0\n",
    "reduce dropout to from 0.7 to 0.5 to 0.1\n",
    "change also 1st layer from elu to tanh which is also LSTM default\n",
    "increase n_hidden from 100 to 300\n",
    "decrease batch_size to 1000 -> GPU RAM at abou 60%\n",
    "lr 0.01 -> overfitting\n",
    "try lr = 0.005 -> worse -> increase n_hidden to 400\n",
    "lr to 0.01\n",
    "increase batch norm momentum to 0.9 -> very unstable\n",
    "decrease batch norm momentum to 0.4 -> very unstable\n",
    "no droout -> unstable\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
